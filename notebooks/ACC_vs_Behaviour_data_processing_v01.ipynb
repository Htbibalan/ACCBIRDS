{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47cc4321",
   "metadata": {},
   "source": [
    "# Here I am trying to merge/align the behavioural anotation logs with ACC logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa02276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations: 9\n",
      "Annotation window: 2025-10-11 11:44:01.133333+00:00 to 2025-10-11 11:45:48.666667+00:00\n",
      "Done.\n",
      "Rows written: 10813\n",
      "Output saved to: ../src/test_data/acc_labeled_testwindow.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ACC_PATH = \"../src/test_data/AlgorithmData-Acceleration-[20251001-20251031]-part2.csv\"\n",
    "ANN_PATH = \"../src/test_data/11-44-01_2a_annotations.csv\"\n",
    "OUT_PATH = \"../src/test_data/acc_labeled_testwindow.csv\"\n",
    "\n",
    "ACC_TIME_COL = \"Collecting time\"\n",
    "ANN_START_COL = \"start_datetime\"\n",
    "ANN_END_COL = \"end_datetime\"\n",
    "ANN_BEHAVIOR_COL = \"behavior\"\n",
    "\n",
    "def to_utc(dt_series):\n",
    "    # Always produce tz-aware UTC timestamps\n",
    "    return pd.to_datetime(dt_series, errors=\"coerce\", utc=True)\n",
    "\n",
    "# first load the annoations and convert to UTC tz-aware timestamps\n",
    "ann = pd.read_csv(ANN_PATH)\n",
    "ann[ANN_START_COL] = to_utc(ann[ANN_START_COL])\n",
    "ann[ANN_END_COL] = to_utc(ann[ANN_END_COL])\n",
    "ann = ann.dropna(subset=[ANN_START_COL, ANN_END_COL, ANN_BEHAVIOR_COL]).sort_values(ANN_START_COL).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded annotations:\", len(ann))\n",
    "\n",
    "window_start = ann[ANN_START_COL].min()\n",
    "window_end = ann[ANN_END_COL].max()\n",
    "print(\"Annotation window:\", window_start, \"to\", window_end)\n",
    "\n",
    "# Precompute annotation windows as int64 ns for fast, tz-safe comparisons\n",
    "ann_start_ns = ann[ANN_START_COL].astype(\"int64\").to_numpy()\n",
    "ann_end_ns   = ann[ANN_END_COL].astype(\"int64\").to_numpy()\n",
    "ann_beh      = ann[ANN_BEHAVIOR_COL].astype(str).to_numpy()\n",
    "\n",
    "# processing acc in chunks to avoid memory issues, and labeling each chunk based on the annotation windows\n",
    "chunksize = 250000\n",
    "first = True\n",
    "total_rows = 0\n",
    "\n",
    "for chunk in pd.read_csv(ACC_PATH, chunksize=chunksize):\n",
    "    if ACC_TIME_COL not in chunk.columns:\n",
    "        raise ValueError(f\"Column '{ACC_TIME_COL}' not found in accelerometer file. Found: {list(chunk.columns)}\")\n",
    "\n",
    "    chunk[ACC_TIME_COL] = to_utc(chunk[ACC_TIME_COL])\n",
    "    chunk = chunk.dropna(subset=[ACC_TIME_COL])\n",
    "\n",
    "    # Restrict to annotation time window \n",
    "    chunk = chunk[(chunk[ACC_TIME_COL] >= window_start) & (chunk[ACC_TIME_COL] <= window_end)]\n",
    "    if len(chunk) == 0:\n",
    "        continue\n",
    "\n",
    "    # Convert acc times to int64 ns (tz-safe)\n",
    "    t_ns = chunk[ACC_TIME_COL].astype(\"int64\").to_numpy()\n",
    "\n",
    "    labels = np.full(len(chunk), \"None\", dtype=object)\n",
    "\n",
    "    # Label rows for each annotation bout\n",
    "    for i in range(len(ann)):\n",
    "        mask = (t_ns >= ann_start_ns[i]) & (t_ns <= ann_end_ns[i])\n",
    "        if mask.any():\n",
    "            labels[mask] = ann_beh[i]\n",
    "\n",
    "    chunk[\"behavior_label\"] = labels\n",
    "\n",
    "    chunk.to_csv(OUT_PATH, index=False, mode=\"w\" if first else \"a\", header=first)\n",
    "    first = False\n",
    "    total_rows += len(chunk)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Rows written:\", total_rows)\n",
    "print(\"Output saved to:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff48362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior             start_datetime               end_datetime\n",
      " Sitting 2025-10-11 11:44:01.133333 2025-10-11 11:44:06.733333\n",
      " Sitting 2025-10-11 11:44:09.400000 2025-10-11 11:44:54.266667\n",
      " Sitting 2025-10-11 11:45:29.733333 2025-10-11 11:45:30.200000\n",
      " Sitting 2025-10-11 11:45:34.066667 2025-10-11 11:45:37.200000\n"
     ]
    }
   ],
   "source": [
    "# just need to see the temp reolution of the annotation file, so we can match it in the ACC file before merging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ANN_PATH = \"../src/test_data/11-44-01_2a_annotations.csv\"\n",
    "ann = pd.read_csv(ANN_PATH)\n",
    "\n",
    "# show all columns so we can see exact start/end values\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# filter Sitting rows \n",
    "sitting = ann[ann[\"behavior\"].astype(str).str.lower() == \"sitting\"]\n",
    "\n",
    "print(sitting[[\"behavior\", \"start_datetime\", \"end_datetime\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512aa83",
   "metadata": {},
   "source": [
    "# just making some improvements to the older version, such that we get a single file containing only labeled segments of the ACC and an additional file containing all ACC data with marked segments of annotated behaviour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a767ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations: 9\n",
      "Annotation window: 2025-10-11 11:44:01.133333+00:00 to 2025-10-11 11:45:48.666667+00:00\n",
      "Done.\n",
      "Snippet rows written: 10813\n",
      "Snippet output saved to: ../src/test_data/acc_labeled_testwindow.csv\n",
      "Full rows written: 1007474\n",
      "Full output saved to: ../src/test_data/acc_with_behavior_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ACC_PATH = \"../src/test_data/AlgorithmData-Acceleration-[20251001-20251031]-part2.csv\"\n",
    "ANN_PATH = \"../src/test_data/11-44-01_2a_annotations.csv\"\n",
    "\n",
    "OUT_SNIPPET_PATH = \"../src/test_data/acc_labeled_testwindow.csv\"\n",
    "OUT_FULL_PATH    = \"../src/test_data/acc_with_behavior_full.csv\"\n",
    "\n",
    "ACC_TIME_COL = \"Collecting time\"\n",
    "ANN_START_COL = \"start_datetime\"\n",
    "ANN_END_COL = \"end_datetime\"\n",
    "ANN_BEHAVIOR_COL = \"behavior\"\n",
    "\n",
    "def to_utc(dt_series):\n",
    "    # \n",
    "    return pd.to_datetime(dt_series, errors=\"coerce\", utc=True)\n",
    "\n",
    "\n",
    "ann = pd.read_csv(ANN_PATH)\n",
    "ann[ANN_START_COL] = to_utc(ann[ANN_START_COL])\n",
    "ann[ANN_END_COL] = to_utc(ann[ANN_END_COL])\n",
    "ann = ann.dropna(subset=[ANN_START_COL, ANN_END_COL, ANN_BEHAVIOR_COL]).sort_values(ANN_START_COL).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded annotations:\", len(ann))\n",
    "if len(ann) == 0:\n",
    "    raise ValueError(\"No valid annotations found after parsing datetimes/behavior.\")\n",
    "\n",
    "window_start = ann[ANN_START_COL].min()\n",
    "window_end = ann[ANN_END_COL].max()\n",
    "print(\"Annotation window:\", window_start, \"to\", window_end)\n",
    "\n",
    "# Precompute annotation windows as int64 ns for fast, tz-safe comparisons\n",
    "ann_start_ns = ann[ANN_START_COL].astype(\"int64\").to_numpy()\n",
    "ann_end_ns   = ann[ANN_END_COL].astype(\"int64\").to_numpy()\n",
    "ann_beh      = ann[ANN_BEHAVIOR_COL].astype(str).to_numpy()\n",
    "\n",
    "# # # # #####\n",
    "chunksize = 250000\n",
    "\n",
    "first_snip = True\n",
    "first_full = True\n",
    "rows_written_snip = 0\n",
    "rows_written_full = 0\n",
    "\n",
    "for chunk in pd.read_csv(ACC_PATH, chunksize=chunksize):\n",
    "    if ACC_TIME_COL not in chunk.columns:\n",
    "        raise ValueError(f\"Column '{ACC_TIME_COL}' not found in accelerometer file. Found: {list(chunk.columns)}\")\n",
    "\n",
    "    chunk[ACC_TIME_COL] = to_utc(chunk[ACC_TIME_COL])\n",
    "    chunk = chunk.dropna(subset=[ACC_TIME_COL])\n",
    "    if len(chunk) == 0:\n",
    "        continue\n",
    "\n",
    "    # Convert acc times to int64 ns (tz-safe)\n",
    "    t_ns = chunk[ACC_TIME_COL].astype(\"int64\").to_numpy()\n",
    "\n",
    "    # Default label = NA \n",
    "    labels_full = np.full(len(chunk), np.nan, dtype=object)\n",
    "\n",
    "    # Label rows for each annotation bout\n",
    "    for i in range(len(ann)):\n",
    "        mask = (t_ns >= ann_start_ns[i]) & (t_ns <= ann_end_ns[i])\n",
    "        if mask.any():\n",
    "            labels_full[mask] = ann_beh[i]\n",
    "\n",
    "    # full FILE\n",
    "    chunk_full = chunk.copy()\n",
    "    chunk_full[\"behavior_label\"] = labels_full\n",
    "    chunk_full.to_csv(\n",
    "        OUT_FULL_PATH,\n",
    "        index=False,\n",
    "        mode=\"w\" if first_full else \"a\",\n",
    "        header=first_full\n",
    "    )\n",
    "    first_full = False\n",
    "    rows_written_full += len(chunk_full)\n",
    "\n",
    "    #\n",
    "    in_window = (chunk[ACC_TIME_COL] >= window_start) & (chunk[ACC_TIME_COL] <= window_end)\n",
    "    if in_window.any():\n",
    "        chunk_snip = chunk.loc[in_window].copy()\n",
    "        # reuse the already computed labels (subset them to the window)\n",
    "        chunk_snip[\"behavior_label\"] = np.asarray(labels_full, dtype=object)[in_window.to_numpy()]\n",
    "        chunk_snip.to_csv(\n",
    "            OUT_SNIPPET_PATH,\n",
    "            index=False,\n",
    "            mode=\"w\" if first_snip else \"a\",\n",
    "            header=first_snip\n",
    "        )\n",
    "        first_snip = False\n",
    "        rows_written_snip += len(chunk_snip)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Snippet rows written:\", rows_written_snip)\n",
    "print(\"Snippet output saved to:\", OUT_SNIPPET_PATH)\n",
    "print(\"Full rows written:\", rows_written_full)\n",
    "print(\"Full output saved to:\", OUT_FULL_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185b458",
   "metadata": {},
   "source": [
    "# Just in case we need to modulate time format of the collection time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68854f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ..\\src\\test_data\\AlgorithmData-Acceleration-[20251001-20251031]-part2.csv\n",
      "Output: ..\\src\\test_data\\AlgorithmData-Acceleration-[20251001-20251031]-part2_SHIFTED_Z.csv\n",
      "Shift hours: 1\n",
      "Total rows: 1008461\n",
      "Unparseable timestamps (left blank): 0\n",
      "\n",
      "Preview (first 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collecting time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-10-11T12:40:58.055Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-10-11T12:40:58.064Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-10-11T12:40:58.074Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-10-11T12:40:58.084Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-10-11T12:40:58.094Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-10-11T12:40:58.104Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-10-11T12:40:58.114Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-10-11T12:40:58.124Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-10-11T12:40:58.134Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-10-11T12:40:58.144Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Collecting time\n",
       "0  2025-10-11T12:40:58.055Z\n",
       "1  2025-10-11T12:40:58.064Z\n",
       "2  2025-10-11T12:40:58.074Z\n",
       "3  2025-10-11T12:40:58.084Z\n",
       "4  2025-10-11T12:40:58.094Z\n",
       "5  2025-10-11T12:40:58.104Z\n",
       "6  2025-10-11T12:40:58.114Z\n",
       "7  2025-10-11T12:40:58.124Z\n",
       "8  2025-10-11T12:40:58.134Z\n",
       "9  2025-10-11T12:40:58.144Z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved âœ…\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# here is where fernando can adjust the file path and the hours need to shift\n",
    "IN_PATH = Path(r\"../src/test_data/AlgorithmData-Acceleration-[20251001-20251031]-part2.csv\")   \n",
    "OUT_PATH = IN_PATH.with_name(IN_PATH.stem + \"_SHIFTED_Z.csv\")\n",
    "\n",
    "TIME_COL = \"Collecting time\"  \n",
    "SHIFT_HOURS = 1                # +1 forward, -2 backward, etc.\n",
    "\n",
    "\n",
    "ALLOW_BAD_ROWS = True\n",
    "\n",
    "\n",
    "_iso_z_re_ms_or_more = re.compile(r\"^(?P<prefix>.*\\.(?P<ms>\\d{3}))(?P<extra>\\d*)Z$\")\n",
    "_iso_has_fraction = re.compile(r\"^(.*\\.)\\d+(Z)$\")\n",
    "\n",
    "def normalise_to_iso_z(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    x = str(x).strip()\n",
    "    if x == \"\" or x.lower() == \"nan\":\n",
    "        return None\n",
    "\n",
    "    # Standardise separator\n",
    "    x = x.replace(\" \", \"T\")\n",
    "\n",
    "    # Convert UTC offsets to Z (handles +00:00 and +0000)\n",
    "    if x.endswith(\"+00:00\"):\n",
    "        x = x[:-6] + \"Z\"\n",
    "    elif x.endswith(\"+0000\"):\n",
    "        x = x[:-5] + \"Z\"\n",
    "    elif x.endswith(\"Z\") is False:\n",
    "        # If it has some other timezone, we still try parsing directly later,\n",
    "        # but most of the data is UTC seemingly\n",
    "        pass\n",
    "\n",
    "    # Ensure a fractional part exists (milliseconds). If none, add .000\n",
    "    if \"T\" in x and (\".\" not in x) and x.endswith(\"Z\"):\n",
    "        x = x[:-1] + \".000Z\"\n",
    "\n",
    "    # If fraction exists but is shorter than 3, pad to 3\n",
    "    if x.endswith(\"Z\") and \".\" in x:\n",
    "        # Split just before Z\n",
    "        base = x[:-1]\n",
    "        left, frac = base.split(\".\", 1)\n",
    "        # keep only digits in frac\n",
    "        frac_digits = re.sub(r\"\\D\", \"\", frac)\n",
    "        if len(frac_digits) >= 3:\n",
    "            frac3 = frac_digits[:3]\n",
    "        else:\n",
    "            frac3 = frac_digits.ljust(3, \"0\")\n",
    "        x = f\"{left}.{frac3}Z\"\n",
    "\n",
    "    # If more than 3 digits exist before Z (microseconds), trim to 3\n",
    "    m = _iso_z_re_ms_or_more.match(x)\n",
    "    if m:\n",
    "        x = m.group(\"prefix\") + \"Z\"\n",
    "\n",
    "    return x\n",
    "\n",
    "def format_iso_z_from_utc(dt_utc: pd.Series) -> pd.Series:\n",
    "    # dt_utc is tz-aware UTC\n",
    "    # %f = microseconds (6 digits). Slice to milliseconds.\n",
    "    return dt_utc.dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\").str.slice(0, 23) + \"Z\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(IN_PATH)\n",
    "if TIME_COL not in df.columns:\n",
    "    raise KeyError(f\"Column '{TIME_COL}' not found. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "raw = df[TIME_COL].astype(\"string\")\n",
    "\n",
    "\n",
    "norm = raw.map(normalise_to_iso_z)\n",
    "\n",
    "# Parse as UTC. If strings already have Z -> ok. If some have +00:00 we normalised to Z.\n",
    "dt = pd.to_datetime(norm, errors=\"coerce\", utc=True)\n",
    "\n",
    "bad_mask = dt.isna() & raw.notna() & (raw.str.strip() != \"\") & (raw.str.lower() != \"nan\")\n",
    "bad_count = int(bad_mask.sum())\n",
    "\n",
    "if bad_count and not ALLOW_BAD_ROWS:\n",
    "    cols_to_show = [c for c in [\"UUID\", \"Transmitting time\", TIME_COL] if c in df.columns]\n",
    "    display(df.loc[bad_mask, cols_to_show].head(50))\n",
    "    raise ValueError(\n",
    "        f\"{bad_count} rows in '{TIME_COL}' could not be parsed. \"\n",
    "        \"Set ALLOW_BAD_ROWS=True to keep them blank, or inspect the displayed rows.\"\n",
    "    )\n",
    "\n",
    "\n",
    "dt_shift = dt + pd.to_timedelta(SHIFT_HOURS, unit=\"h\")\n",
    "\n",
    "\n",
    "shifted_str = format_iso_z_from_utc(dt_shift)\n",
    "\n",
    "# Keep blanks where parsing failed / original was blank\n",
    "shifted_str = shifted_str.where(~dt_shift.isna(), other=\"\")\n",
    "\n",
    "# Write back\n",
    "df[TIME_COL] = shifted_str\n",
    "\n",
    "\n",
    "print(\"Input:\", IN_PATH)\n",
    "print(\"Output:\", OUT_PATH)\n",
    "print(\"Shift hours:\", SHIFT_HOURS)\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Unparseable timestamps (left blank):\", bad_count)\n",
    "\n",
    "print(\"\\nPreview (first 10):\")\n",
    "display(df[[TIME_COL]].head(10))\n",
    "\n",
    "df.to_csv(OUT_PATH, index=False)\n",
    "print(\"\\nSaved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7c4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACCBIRDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
